- [ ] Create a new git repo for the extractor project.
- [ ] Install Python 3.10 and create a dedicated virtual environment.
- [ ] Install PyTorch CPU build and Hugging Face Transformers.
- [ ] Install datasets, accelerate, peft, evaluate, optimum, onnxruntime.
- [ ] Install Label Studio locally for secure annotation.
- [ ] Install pypdf, pymupdf, rapidocr, tesseract optional, and regex toolkit.
- [ ] Install scikit learn, numpy, pandas, matplotlib for utilities.
- [ ] Install dateparser, phonenumbers, usaddress for normalization utilities.
- [ ] Initialize a .env with absolute project paths.
- [ ] Write script to batch extract text per page from PDFs.
- [ ] Preserve page order and store per page plaintext files.
- [ ] Optionally capture token bounding boxes for layout models later.
- [ ] Define final entity schema exactly matching your output fields.
- [ ] Create mapping table from schema to BIO tag set.
- [ ] Spin up Label Studio with project for token span annotation.
- [ ] Annotate twenty representative contracts covering all fields and edge cases.
- [ ] Export annotations as JSON and keep raw texts per page.
- [ ] Write converter producing token level BIO labels using tokenizer alignment.
- [ ] Generate weak labels with regex for phones, emails, money, dates.
- [ ] Auto label obvious fields using keywords like Buyer, Seller, Listing Agent.
- [ ] Pretrain briefly on weak labels; fine tune using gold annotations.
- [ ] Create simple text templates for purchase agreements and addenda.
- [ ] Populate templates with randomized realistic names, addresses, phone formats, prices.
- [ ] Programmatically insert schema labels within generated texts for supervision.
- [ ] Mix synthetic samples with annotated contracts, oversampling real contracts twofold.
- [ ] Increase loss weight for private contracts using sample weights during training.
- [ ] Split dataset into train, validation, and test by contract, not pages.
- [ ] Chunk long pages into 512 token windows with 64 overlap.
- [ ] Keep page and offset metadata for merging predictions later.
- [ ] Balance classes using class weights computed from training labels.
- [ ] Save datasets in Hugging Face DatasetDict format for Trainer.
- [ ] Load Contracts BERT tokenizer and model for token classification.
- [ ] Replace classification head with your label count and label id map.
- [ ] Enable gradient accumulation to keep batch small on 8GB VRAM.
- [ ] Use AdamW optimizer, linear warmup, early stopping on validation F1.
- [ ] Apply layer wise learning rate decay for stable fine tuning.
- [ ] Train three to five epochs, monitor per label precision and recall.
- [ ] Evaluate on held out contracts, inspect confusion between similar roles.
- [ ] If truncation harms recall, try Longformer base for 4096 tokens.
- [ ] Otherwise keep chunked Contracts BERT with sliding windows.
- [ ] Run model on unlabeled contracts and flag low confidence spans.
- [ ] Manually correct flagged spans in Label Studio to strengthen supervision.
- [ ] Retrain briefly on updated labels; repeat until validation stabilizes.
- [ ] Quantize model to INT8 with Optimum for faster CPU inference.
- [ ] Optionally export to ONNX Runtime for additional CPU speedups.
- [ ] Benchmark inference time per page, target under one second.
- [ ] Benchmark full document latency; enforce under thirty seconds constraint.
- [ ] Parallelize pages across CPU cores using multiprocessing with memory limits.
- [ ] Merge per page predictions into document level entity lists.
- [ ] Resolve duplicates by confidence, position, and nearest label keywords.
- [ ] Derive BYR1REL1 equals 'and' only when multiple buyer names found.
- [ ] Leave BYR1NAM2 blank when only one buyer span detected.
- [ ] Format phone numbers to '(###)###-####' consistently.
- [ ] Normalize money fields removing symbols and commas; keep digits only.
- [ ] Standardize dates to 'MM/DD/YYYY' using robust dateparser.
- [ ] Use usaddress to split street, city, state, and zip components.
- [ ] Set static fields exactly as specified for unchanged values.
- [ ] Compute LORU as 'Lot' when lot present; else 'Unit' when unit.
- [ ] Sum commission percentages into COMPCT, strip percent characters.
- [ ] Set PLISTINGAGENT to zero unless explicitly stated otherwise.
- [ ] Validate license numbers using regex patterns for typical state formats.
- [ ] Assemble final dictionary with all required schema fields.
- [ ] Write results to `.pxt` file per contract in output folder.
- [ ] Include JSON alongside `.pxt` for debugging and audits.
- [ ] Compute per field precision, recall, and F1 on test set.
- [ ] Log extraction failures and examples to a review dashboard.
- [ ] Add rules catching impossible values before writing outputs.
- [ ] Schedule periodic retraining when error rate drifts upward.
- [ ] Create CLI script: extractor input.pdf --out outdir.
- [ ] Add batch mode accepting folder paths and processing concurrently.
- [ ] Package into Windows executable using PyInstaller for distribution.
- [ ] Version datasets and models with DVC for reproducibility.
- [ ] If layout needed, store OCR coordinates and page images.
- [ ] Fine tune LayoutLMv3 with coordinates and text labels.
- [ ] Fallback to text only model if layout gains negligible.
- [ ] For stubborn fields, add lightweight QA prompts per field.
- [ ] Use QA answers only when NER confidence falls below threshold.
- [ ] Keep all data local; disable telemetry and cloud features.
- [ ] Encrypt datasets at rest; restrict access to project directory.
- [ ] Document everything in README with exact versions and commands.